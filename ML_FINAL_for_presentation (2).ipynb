{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Project- Chess games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Idan Vazana, 204154207\n",
    "#### Karin Tatzat, 201048691\n",
    "#### Keren Kaplan, 205681646"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import & Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9544550a1dac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_objs\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mplotly\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, classification_report, confusion_matrix, plot_confusion_matrix\n",
    "#from sklearn.metrics import classification_report, confusion_matrix\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "from string import ascii_letters\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import chi2_contingency, f_oneway\n",
    "\n",
    "from category_encoders.count import CountEncoder\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from yellowbrick.cluster import KElbowVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/idanvazana/Desktop/games.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling with duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### we saw that there are duplicates rows with the same id game \n",
    "###### we decided to remove them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create an integer columns for black and white id and game id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['white_id_int'] = pd.factorize(df['white_id'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['black_id_int'] = pd.factorize(df['black_id'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['id_int'] = pd.factorize(df['id'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create a column that calculate the difference between the black and white rating:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that the bigger the difference, the higher the chance for the higher ranked to win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rating_difference'] = df['white_rating'] - df['black_rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### create a column - opening pref based on opening name: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing opening name to two words for grouping\n",
    "df['opening_pref'] = df['opening_name'].apply(lambda x: ' '.join(x.split(' ')[:2]))\n",
    "df['opening_pref'] = df['opening_pref'].apply(lambda x: x[:-1] if str(x).endswith(':') else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create a column time_control based on increment time  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_control(df):\n",
    "    time_control = [int(x) for x in df['increment_code'].split('+')]\n",
    "    return time_control[0] + np.floor((time_control[1] * df['turns']/2) / 60)\n",
    "df['time_control'] = df.apply(time_control, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create 5 new columns for the first moves and save into new dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split 'moves' column into 5 new columns for the first 5 moves:\n",
    "df = df.assign(move1=df['moves'].str.split(\" \").str[0],\n",
    "               move2=df['moves'].str.split(\" \").str[1],\n",
    "               move3=df['moves'].str.split(\" \").str[2],\n",
    "               move4=df['moves'].str.split(\" \").str[3],\n",
    "               move5=df['moves'].str.split(\" \").str[4])\n",
    "\n",
    "# Extract the first five elements from the 'moves' column\n",
    "#df = pd.concat([df.drop(['moves'], axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dropping draw column and values, to fit problem into binnary classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['winner'].isin(['draw'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dropping dates columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['created_at', 'last_move_at'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create dummies dataset for the 5 first moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies = pd.get_dummies(df, columns=['winner', 'victory_status', 'move1', 'move2', 'move3', 'move4', 'move5'], drop_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_dummies.shape)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = pd.merge(df_dummies, df[['id_int','winner', 'victory_status', 'move1', 'move2', 'move3', 'move4', 'move5']], on = 'id_int', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns explanation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sum values numbers for each feature\n",
    "for column in df:\n",
    "    try: \n",
    "        unique_vals = np.unique(df[column])\n",
    "        nr_values = len(unique_vals)\n",
    "        if nr_values < 5:\n",
    "            print ('The number of values for feature {} : {} -- {}'.format(column, nr_values,unique_vals))\n",
    "        else:\n",
    "            print ('The number of values for feature {} : {}'.format(column, nr_values))\n",
    "    except:\n",
    "        print (\"Columns \",column, \" contains Null Values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['winner'].unique())\n",
    "print(df['victory_status'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One dimentional analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  rated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rated = df['rated'].value_counts().reset_index()\n",
    "df_rated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = df['rated'].value_counts()\n",
    "value_counts_percent = value_counts / len(df) * 100\n",
    "value_counts_percent.plot(kind='bar')\n",
    "plt.title('Percentage of Value Counts')\n",
    "plt.xlabel('Number of rated')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_features = ['rated']\n",
    "label = 'winner'\n",
    "\n",
    "dic = {'Categorical': [],\n",
    "    'Numerical': [],\n",
    "    'p-value': [],\n",
    "    'p < 0.05': [],\n",
    "    'statistic': []}\n",
    "\n",
    "\n",
    "for feature in cont_features:\n",
    "    values = []\n",
    "    for value in df[label].unique():\n",
    "        values.append(df[df[label] == value][feature].values)\n",
    "    \n",
    "    statistic, pval = f_oneway(*values)\n",
    "    \n",
    "    dic['Categorical'].append(label)\n",
    "    dic['Numerical'].append(feature)\n",
    "    dic['p-value'].append(pval)\n",
    "    dic['p < 0.05'].append(pval<0.05)\n",
    "    dic['statistic'].append(statistic)\n",
    "\n",
    "\n",
    "pd.DataFrame(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The rated feature isn't a good predictor of the winner - because the p-valueis more than 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### turnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['turns'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, x='turns', orient=\"h\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_turns = pd.DataFrame({\"turns\": df['turns']},columns=[\"turns\"])\n",
    "df_turns.plot.hist(alpha=0.3, bins=15,color='turquoise');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_features = ['turns']\n",
    "label = 'winner'\n",
    "\n",
    "dic = {'Categorical': [],\n",
    "    'Numerical': [],\n",
    "    'p-value': [],\n",
    "    'p < 0.05': [],\n",
    "    'statistic': []}\n",
    "\n",
    "\n",
    "for feature in cont_features:\n",
    "    values = []\n",
    "    for value in df[label].unique():\n",
    "        values.append(df[df[label] == value][feature].values)\n",
    "    \n",
    "    statistic, pval = f_oneway(*values)\n",
    "    \n",
    "    dic['Categorical'].append(label)\n",
    "    dic['Numerical'].append(feature)\n",
    "    dic['p-value'].append(pval)\n",
    "    dic['p < 0.05'].append(pval<0.05)\n",
    "    dic['statistic'].append(statistic)\n",
    "\n",
    "\n",
    "pd.DataFrame(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The \"turns\" feature is a good predictor of the winner - because the p-value is under 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rating_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rating_difference.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.hist(df['rating_difference'],alpha=0.6)\n",
    "plt.title(\"Difference between white rating and black rating\")\n",
    "plt.xlabel('difference')\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As wee see, in most cases, games are relatively fair (both players have similar rating). But there is decent number of games where the discrepancy is relatively large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df\n",
    "feature_1 = 'winner'\n",
    "feature_2 = 'rating_difference'\n",
    "plt.figure(figsize=(7,7))\n",
    "sns.boxplot(x=feature_1, y=feature_2, data=dataframe)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In rating_difference parameter we see a slight tendency towards the white player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_features = ['rating_difference']\n",
    "label = 'winner'\n",
    "\n",
    "dic = {'Categorical': [],\n",
    "    'Numerical': [],\n",
    "    'p-value': [],\n",
    "    'p < 0.05': [],\n",
    "    'statistic': []}\n",
    "\n",
    "\n",
    "for feature in cont_features:\n",
    "    values = []\n",
    "    for value in df[label].unique():\n",
    "        values.append(df[df[label] == value][feature].values)\n",
    "    \n",
    "    statistic, pval = f_oneway(*values)\n",
    "    \n",
    "    dic['Categorical'].append(label)\n",
    "    dic['Numerical'].append(feature)\n",
    "    dic['p-value'].append(pval)\n",
    "    dic['p < 0.05'].append(pval<0.05)\n",
    "    dic['statistic'].append(statistic)\n",
    "\n",
    "\n",
    "pd.DataFrame(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The difference between ratings is indeed a good predictor of the winner - because the p-value is under 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### white and black rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['white_rating','black_rating']].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['white_rating'], bins=10, alpha=0.5, label='white')\n",
    "plt.hist(df['black_rating'], bins=10, alpha=0.5, label='black')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_features = ['white_rating']\n",
    "label = 'winner'\n",
    "\n",
    "dic = {'Categorical': [],\n",
    "    'Numerical': [],\n",
    "    'p-value': [],\n",
    "    'p < 0.05': [],\n",
    "    'statistic': []}\n",
    "\n",
    "\n",
    "for feature in cont_features:\n",
    "    values = []\n",
    "    for value in df[label].unique():\n",
    "        values.append(df[df[label] == value][feature].values)\n",
    "    \n",
    "    statistic, pval = f_oneway(*values)\n",
    "    \n",
    "    dic['Categorical'].append(label)\n",
    "    dic['Numerical'].append(feature)\n",
    "    dic['p-value'].append(pval)\n",
    "    dic['p < 0.05'].append(pval<0.05)\n",
    "    dic['statistic'].append(statistic)\n",
    "\n",
    "\n",
    "pd.DataFrame(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The \"white_rating\" feature is a good predictor of the winner - because the p-value is under 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_features = ['black_rating']\n",
    "label = 'winner'\n",
    "\n",
    "dic = {'Categorical': [],\n",
    "    'Numerical': [],\n",
    "    'p-value': [],\n",
    "    'p < 0.05': [],\n",
    "    'statistic': []}\n",
    "\n",
    "\n",
    "for feature in cont_features:\n",
    "    values = []\n",
    "    for value in df[label].unique():\n",
    "        values.append(df[df[label] == value][feature].values)\n",
    "    \n",
    "    statistic, pval = f_oneway(*values)\n",
    "    \n",
    "    dic['Categorical'].append(label)\n",
    "    dic['Numerical'].append(feature)\n",
    "    dic['p-value'].append(pval)\n",
    "    dic['p < 0.05'].append(pval<0.05)\n",
    "    dic['statistic'].append(statistic)\n",
    "\n",
    "\n",
    "pd.DataFrame(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The \"black_rating\" feature is a good predictor of the winner - because the p-value is under 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### victory_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['victory_status'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = df['victory_status'].value_counts()\n",
    "value_counts_percent = value_counts / len(df) * 100\n",
    "value_counts_percent.plot(kind='bar')\n",
    "plt.title('Percentage of Value Counts')\n",
    "plt.xlabel('Number of victory_status')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(6, 6)})\n",
    "sns.countplot(data=df, x='victory_status', hue='winner', palette='inferno').set(title='Counts of Victory Status')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### opening_pref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['opening_pref'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = df['opening_pref'].value_counts()\n",
    "value_counts = value_counts.head(10)\n",
    "value_counts_percent = value_counts / len(df) * 100\n",
    "value_counts_percent.plot(kind='bar')\n",
    "plt.title('Percentage of Value Counts')\n",
    "plt.xlabel('Number of opening_pref')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['opening_eco','opening_pref']].loc[df['opening_pref'] == 'French Defense'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['opening_eco'] == 'C00')][['opening_eco','opening_pref']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### opening eco and opening pref which based on opening name are related but also has multiple values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### time_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time_control'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.hist(df['time_control'],alpha=0.6)\n",
    "plt.title(\"Distirution of time_control\")\n",
    "plt.xlabel('time_control')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df\n",
    "feature_1 = 'winner'\n",
    "feature_2 = 'time_control'\n",
    "plt.figure(figsize=(7,7))\n",
    "sns.boxplot(x=feature_1, y=feature_2, data=dataframe)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_features = ['time_control']\n",
    "label = 'winner'\n",
    "\n",
    "dic = {'Categorical': [],\n",
    "    'Numerical': [],\n",
    "    'p-value': [],\n",
    "    'p < 0.05': [],\n",
    "    'statistic': []}\n",
    "\n",
    "\n",
    "for feature in cont_features:\n",
    "    values = []\n",
    "    for value in df[label].unique():\n",
    "        values.append(df[df[label] == value][feature].values)\n",
    "    \n",
    "    statistic, pval = f_oneway(*values)\n",
    "    \n",
    "    dic['Categorical'].append(label)\n",
    "    dic['Numerical'].append(feature)\n",
    "    dic['p-value'].append(pval)\n",
    "    dic['p < 0.05'].append(pval<0.05)\n",
    "    dic['statistic'].append(statistic)\n",
    "\n",
    "\n",
    "pd.DataFrame(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The time_control feature isn't a good predictor of the winner - because the p-valueis more than 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['winner'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['winner'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(6, 6)})\n",
    "sns.countplot(data=df, x='winner', palette='inferno').set(title='Counts of winner')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can see that the data is balanced- white's number of wins is similiar to black's number of wins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df[['white_rating','black_rating', 'turns','rating_difference','time_control', 'winner']].groupby(\"winner\").describe().T\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df_merge[['turns','opening_ply','victory_status_mate', 'victory_status_outoftime','victory_status_resign',\n",
    "                     'white_rating', 'black_rating', 'rating_difference','time_control','winner_black', 'winner_white']]\n",
    "\n",
    "corr_matrix = df_corr.corr()\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='YlGnBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the rating variable has an effect on the outcome of the game. The higher the player's rating, the greater the chance he has of winning.\n",
    "As a result, the rating_difference variable also affects the outcome of the game. The greater the difference between the players' ratings, the higher the chance of the player with the higher rating to win."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the relationship between numeric features to the winner:\n",
    "df_corr = df_merge[['turns','white_rating','black_rating', 'opening_ply','time_control', 'rating_difference', 'winner']]\n",
    "g = sns.pairplot(df_corr, hue ='winner', diag_kws={'bw': 0.2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this graph we can see how the different features divide the data into Black's victory / White's victory.\n",
    "\n",
    "##### we can see that the features: rating difference divide the data very well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We want to take the features that have low cardinality -\n",
    "In machine learning, \"cardinality\" refers to the number of unique values in a feature or column of a data set. Features with high cardinality have a large number of unique values, making them more difficult to encode and process for many machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cardinality checking:\n",
    "\n",
    "def check_cardinality(df):\n",
    "    for column in df.columns:\n",
    "        cardinality = df[column].nunique()\n",
    "        print(\"The cardinality of the feature '{}' is: {}\".format(column, cardinality))\n",
    "check_cardinality(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features we will use: 'rated', 'victory_status', 'turns', 'white_rating', 'black_rating', 'opening_ply', 'rating_diff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_card_df = df[['rated','victory_status',\n",
    "                   'turns','white_rating', \n",
    "                   'black_rating', 'opening_ply', \n",
    "                   'rating_difference']].copy()\n",
    "\n",
    "#Process categorical features\n",
    "low_card_df['rated'] = low_card_df['rated'].map({False: 0, True:1})\n",
    "low_card_df = pd.get_dummies(low_card_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Recall, precision, and F1 score are three commonly used metrics to evaluate the performance of a binary classifier.\n",
    "Recall - Recall is the proportion of positive instances that are correctly identified by the classifier. It measures the ability of the classifier to find all positive instances. Recall is calculated as TP / (TP + FN).\n",
    "\n",
    "Precision - Precision is the proportion of positive instances that are correctly classified by the classifier. It measures the ability of the classifier to avoid false positive instances. Precision is calculated as TP / (TP + FP).\n",
    "\n",
    "F1 Score - is the harmonic mean of precision and recall. It provides a single value that summarizes the precision and recall of the classifier. The F1 score is calculated as 2 * (Precision * Recall) / (Precision + Recall). A high F1 score indicates that the classifier has a good balance of precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = low_card_df\n",
    "y = df['winner']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# create an instance of the DecisionTreeClassifier class\n",
    "dtc = tree.DecisionTreeClassifier(criterion=\"gini\", max_depth=3, min_samples_split=100)\n",
    "\n",
    "\n",
    "# fit the classifier to the data\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dtc.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(200,100))\n",
    "plot_tree(dtc, feature_names=X_train.columns, class_names=['white', 'black'], filled=True, rounded=True);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_test,y_pred)\n",
    "plt.show()\n",
    "\n",
    "tree_clf = tree.DecisionTreeClassifier().fit(X_train, y_train)\n",
    "print(classification_report(y_true=y_test, y_pred=tree_clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We saw that out of 1954 observations, the model was able to correctly predict 1522 observations - 78%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we want to see if the 5 first move can indicates the winner of the game. so, now we choose the rellevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_col = df_dummies[[col for col in df_dummies.columns if col.startswith('move')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_col.drop(['moves'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_col.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = df_rel_col\n",
    "y1 = df['winner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1,X_test1,y_train1,y_test1 = train_test_split(X1,y1,test_size=1/3,random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_random_state = None\n",
    "log_clf = LogisticRegression(random_state=log_random_state,max_iter=500).fit(X_train1, y_train1)\n",
    "print(classification_report(y_true=y_test1, y_pred=log_clf.predict(X_test1)))\n",
    "plot_confusion_matrix(log_clf, X_test1, y_test1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clusters = df[['turns','victory_status','winner','increment_code','white_rating','black_rating','opening_pref', 'time_control','rating_difference','moves','opening_eco','opening_name','opening_ply']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = df_clusters[['turns','rating_difference','opening_eco','opening_pref','opening_ply','time_control']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CountEncoder()\n",
    "\n",
    "clusters[['op_name','op_eco']] = encoder.fit_transform(clusters[['opening_pref','opening_eco']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "features = ['turns','op_name','op_eco','opening_ply','rating_difference','time_control']\n",
    "X = scaler.fit_transform(clusters[features])\n",
    "X_processed = pd.DataFrame(X, columns = features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### choose the optimum K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(random_state=0)\n",
    "# Compute cluster centers and predict cluster indices\n",
    "visualizer = KElbowVisualizer(kmeans, k=(2,12))\n",
    "visualizer.fit(X_processed)        # Fit the data to the visualizer\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3,random_state=0)\n",
    "pca_df = pd.DataFrame(pca.fit_transform(X_processed), columns = ['p1','p2','p3'])\n",
    "   \n",
    "kmeans = KMeans(n_clusters=5,random_state=0)\n",
    "\n",
    "# Compute cluster centers and predict cluster indices\n",
    "\n",
    "X_clustered = kmeans.fit_predict(pca_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(X_clustered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters['group'] = X_clustered\n",
    "clusters['group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster0 = clusters[clusters['group']==0]\n",
    "cluster1 = clusters[clusters['group']==1]\n",
    "cluster2 = clusters[clusters['group']==2]\n",
    "cluster3 = clusters[clusters['group']==3]\n",
    "cluster4 = clusters[clusters['group']==4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster0.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### those opening eco are common like sicilian defence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster0['opening_eco'].value_counts()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 23% of the observations are in cluster0 \n",
    "We can see that this group is characterized by a very high number of turns, and a very large rating difference compared to the other groups. Op_name is higher than the other groups, which shows preference for more popular openings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster1.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster1['opening_eco'].value_counts()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 34% of the observations are in cluster1- the biggest group\n",
    "This group does not seem to have any unique characteristics. Apart from a high value in rating difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster2.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster2['opening_eco'].value_counts()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25% of the observations are in cluster2 \n",
    "The most represented ECO code here is A00, which is an uncommon opening\n",
    "base on https://www.chessgames.com/perl/chessopening?eco=b00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster3.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster3['opening_eco'].value_counts()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 17% of the observations are in cluster3\n",
    "This group is characterized by a very low rating difference, which means that the games in this group are very fair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster4.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1% of the observations are in cluster4 - the smallest group\n",
    "This group is characterized by a very low number of turns, and a very high time control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we performed various manipulations on the data, as we described in the preparation phase. We used 2 models for prediction and another model for classification. 20% of the data is test data and 80% is training data.\n",
    "The first model: a decision tree - was run with different depths and it is clear that its best performance is with a depth of 3 and the parameters for the division are the ranking differences and the number of turns. Index f1 = 70%.\n",
    "The second model: logistic regression - with the help of this model we tried to predict with the help of the first 5 moves whether each of them can in a certain way predict the winner of the game.\n",
    "The explained variable: the winner of the game\n",
    "The explanatory variables: the first 5 moves (each separately)\n",
    "With the help of this model we reached a score of f1 = 55% and therefore it is not possible to rely on this model in predicting the winner and it is even considered like flipping a coin.\n",
    "The third model: k-means - the goal is division into similar groups. In the initial phase, we normalized the data and performed dimensionality reduction before dividing the groups.\n",
    "From this model we noticed that a division was made into 5 groups. There is a central group whose size is 34%. There is another group of players who are not experienced - it is evident that they use unfamiliar moves. Group number 3 is characterized by fair games.\n",
    "Our personal recommendation is to deepen the method of determining the rating of the players because this is the most prominent variable chosen for prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
